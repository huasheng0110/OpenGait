import torch
from torch.nn import functional as F
import torch.nn as nn
from torchvision.models.resnet import BasicBlock, Bottleneck, ResNet
from ..modules import BasicConv2d
from ..modules import DynamicFeatureBranch, FeatureFusion,ProgressiveFusionBranch, FusionModule,MultiScaleTemporalFusion,PackSequenceWrapper
from ..BasicBlock3D import BasicBlock3D
from ..BasicBlockP3D import BasicBlockP3D

block_map = {
    'BasicBlock3D': BasicBlock3D,
    'BasicBlockP3D': BasicBlockP3D,
    'BasicBlock': BasicBlock,
    'Bottleneck': Bottleneck
}


class ResNet9(ResNet):
    def __init__(self, block, channels=[32, 64, 128, 256], in_channel=1, layers=[1, 2, 2, 1], strides=[1, 2, 2, 1], maxpool=True):
        if block in block_map.keys():
            block = block_map[block]
        else:
            raise ValueError(
                "Error type for -block-Cfg-, supported: 'BasicBlock' or 'Bottleneck'.")
        self.maxpool_flag = maxpool
        super(ResNet9, self).__init__(block, layers)

        # Not used #
        self.fc = None
        ############
        self.inplanes = channels[0]
        self.bn1 = nn.BatchNorm2d(self.inplanes)

        self.conv1 = BasicConv2d(in_channel, self.inplanes, 3, 1, 1)

        self.layer1 = self._make_layer(
            block, channels[0], layers[0], stride=strides[0], dilate=False)

        self.layer2 = self._make_layer(
            block, channels[1], layers[1], stride=strides[1], dilate=False)
        # åœ¨layer2åæ·»åŠ ä¸€ä¸ªæ—¶é—´é™ç»´çš„åˆ†æ”¯ï¼Œç”¨TemporalFusionConvæ¨¡å—
        self.temporal_fusion_conv = TemporalFusionConv(in_channels=channels[1])
        # ç‰¹å¾èåˆæ¨¡å—
        self.feature_fusion = FeatureFusion(in_channels=channels[1])

        self.layer3 = self._make_layer(
            block, channels[2], layers[2], stride=strides[2], dilate=False)
        self.layer4 = self._make_layer(
            block, channels[3], layers[3], stride=strides[3], dilate=False)

    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):
        if blocks >= 1:
            layer = super()._make_layer(block, planes, blocks, stride=stride, dilate=dilate)
        else:
            def layer(x): return x
        return layer

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        if self.maxpool_flag:
            x = self.maxpool(x)

        x = self.layer1(x)
        x_layer2 = self.layer2(x) # ä¿å­˜layer2çš„è¾“å‡º
        B_T, C, W, H = x_layer2.shape
        B = B_T // 30  # T=30
        T = 30
        x_t = x_layer2.view(B, T, C, W, H)
        # ç»è¿‡æ—¶é—´é™ç»´
        x_t = self.temporal_fusion_conv(x_t) # ç»´åº¦å˜ä¸º (B, T/2, C, W, H)

        # å˜å› (B*T/2, C, W, H)
        x_t = x_t.view(B * (T // 2), C, W, H)  #è¿™é‡Œæœ‰å¯èƒ½éœ€è¦ç”¨åˆ°reshapeè€Œä¸æ˜¯view

        # ç‰¹å¾èåˆ
        x_fused = self.feature_fusion(x_layer2, x_t)

        # æ®‹å·®è¿æ¥
        x_residual = x_layer2 + x_fused

        # x = self.layer2(x)
        x = self.layer3(x_residual)
        x = self.layer4(x)
        return x

# """Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·"""
# """ResNet9_3D"""

def pairwise_temporal_diff(x):
    # x: [B, C, T, H, W]
    T = x.shape[2]
    assert T % 2 == 0, "T must be even to pair frames"

    x1 = x[:, :, 0::2, :, :]  # even frames
    x2 = x[:, :, 1::2, :, :]  # odd frames
    return x2 - x1  # [B, C, T//2, H, W]

class ResNet9_3D(nn.Module):
    def __init__(
        self,
        block,
        channels=[64, 128, 256, 512],
        in_channel=1,
        layers=[1, 1, 1, 1],
        strides=[1, (2, 2, 2), (2, 2, 2), 1],  # ğŸ‘ˆ æ”¯æŒ tuple
        norm_layer=None
    ):
        if block in block_map.keys():
            block = block_map[block]
        else:
            raise ValueError(
                "Error type for -block-Cfg-, supported: 'BasicBlock' or 'Bottleneck'.")
        super(ResNet9_3D, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm3d

        self.inplanes = channels[0]

        self.conv1 = nn.Conv3d(in_channel, self.inplanes, kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)

        self.layer1 = self._make_layer(block, channels[0], layers[0], stride=strides[0], norm_layer=norm_layer)
        self.layer2 = self._make_layer(block, channels[1], layers[1], stride=strides[1], norm_layer=norm_layer)
        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=strides[2], norm_layer=norm_layer)
        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=strides[3], norm_layer=norm_layer)


    def _make_layer(self, block, planes, blocks, stride, norm_layer):
        # ğŸ§  è‡ªåŠ¨é€‚é… stride ç±»å‹
        if isinstance(stride, int):
            stride = (1, stride, stride)
        elif isinstance(stride, list):
            stride = tuple(stride)

        downsample = None
        print("blockæ¨¡å—",block)
        if stride != (1, 1, 1) or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv3d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                norm_layer(planes * block.expansion),
            )

        layers = [block(self.inplanes, planes, stride=stride, downsample=downsample, norm_layer=norm_layer)]
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, stride=(1, 1, 1), norm_layer=norm_layer))

        return nn.Sequential(*layers)

    def forward(self, x):  # x: [B, C, T, H, W]
        x = self.relu(self.bn1(self.conv1(x)))
        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        return x

# """Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·"""
# """ResNet9_P3D"""

class ResNet9_P3D(nn.Module):
    def __init__(self, block, channels=[64, 128, 256, 512],
                 in_channel=1, layers=[1, 1, 1, 1], strides=[1, 2, 2, 1],
                 norm_layer=None):
        if block in block_map.keys():
            block = block_map[block]
        else:
            raise ValueError(
                "Error type for -block-Cfg-, supported: 'BasicBlock' or 'Bottleneck'.")
        super(ResNet9_P3D, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm3d

        self.inplanes = channels[0]

        self.conv1 = nn.Conv3d(in_channel, self.inplanes,
                               kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)

        self.layer1 = self._make_layer(block, channels[0], layers[0], stride=strides[0], norm_layer=norm_layer)
        self.layer2 = self._make_layer(block, channels[1], layers[1], stride=strides[1], norm_layer=norm_layer)
        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=strides[2], norm_layer=norm_layer)
        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=strides[3], norm_layer=norm_layer)

#         # # âœ¨ åŠ¨æ€åˆ†æ”¯ï¼Œä½¿ç”¨ä¸»å¹²æŸä¸ª 2D å·ç§¯ä½œä¸ºå…±äº«æ¨¡å—ï¼ˆè¿™é‡Œåªç¤ºä¾‹ç”¨ä¸€ä¸ªæ–°å·ç§¯ï¼‰
#         # shared_conv = nn.Sequential(
#         #     nn.Conv2d(channels[1], channels[1], kernel_size=3, stride=1, padding=1),
#         #     nn.BatchNorm2d(channels[1]),
#         #     nn.ReLU(inplace=True)
#         # )
#         # self.temporal_branch = TemporalDiffBranch(shared_conv)

#         # # âœ¨ ç‰¹å¾èåˆï¼Œæ‹¼æ¥ channel åå‹ç¼©å›ä¸»å¹²é€šé“
#         # self.feature_fusion = FeatureFusion(in_channels=channels[3]*2, out_channels=channels[3])
#                 # å…±äº«ä¸»å¹²ä¸­ conv2d å·ç§¯
        # self.block2_conv = self.layer2[0].conv2d  # ç¬¬ä¸€ä¸ªblockçš„å·ç§¯æ¨¡å—
        # self.block3_conv = self.layer3[0].conv2d
        # self.block4_conv = self.layer4[0].conv2d


#         # self.dynamic_branch = DynamicFeatureBranch(self.block2_conv, self.block3_conv, self.block4_conv)
#         # self.feature_fusion = FeatureFusion(in_channels=channels[3]*2, out_channels=channels[3])
#         self.dynamic_branch = ProgressiveFusionBranch(
#             conv2=self.layer2[0].conv2d,
#             conv3=self.layer3[0].conv2d,
#             conv4=self.layer4[0].conv2d,
#             fuse2=FeatureFusion(128, 128, 128),
#             fuse3=FeatureFusion(256, 256, 256),
#             fuse4=FeatureFusion(512, 512, 512)
#         )


    def _make_layer(self, block, planes, blocks, stride, norm_layer):
         # ğŸ§  è‡ªåŠ¨é€‚é… stride ç±»å‹
        if isinstance(stride, int):
            stride = (1, stride, stride)
        elif isinstance(stride, list):
            stride = tuple(stride)

        downsample = None
        if stride != (1,1,1) or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv3d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                norm_layer(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride=stride,
                            downsample=downsample, norm_layer=norm_layer))
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, stride=(1,1,1), norm_layer=norm_layer))

        return nn.Sequential(*layers)

#     # def forward(self, x):  # x: [B, C, T, H, W]
#     #     x = self.relu(self.bn1(self.conv1(x)))
#     #     x = self.layer1(x)
#     #     # åŠ¨æ€åˆ†æ”¯ä»è¿™é‡Œå¼€å§‹æå–
#     #     x_dyn = self.dynamic_branch(x)

#     #     x = self.layer2(x)
#     #     x = self.layer3(x)
#     #     x = self.layer4(x)

#     #     # èåˆä¸»å¹²ä¸åˆ†æ”¯ç‰¹å¾
#     #     x = self.feature_fusion(x, x_dyn)
#     #     return x
#     def forward(self, x):
#         x = self.relu(self.bn1(self.conv1(x)))

#         x1 = self.layer1(x)
#         x2 = self.layer2(x1)
#         x3 = self.layer3(x2)
#         x4 = self.layer4(x3)

#         # ğŸ¯ ä½¿ç”¨é€å±‚èåˆåˆ†æ”¯æ¨¡å—
#         x_out = self.dynamic_branch(x1, x2, x3, x4)

#         return x_out


class ResNet9_P3D(nn.Module):
    def __init__(self, block, channels=[64, 128, 256, 512],
                 in_channel=1, layers=[1, 1, 1, 1], strides=[1, 2, 2, 1],
                 norm_layer=None):
        if block in block_map.keys():
            block = block_map[block]
        else:
            raise ValueError(
                "Error type for -block-Cfg-, supported: 'BasicBlock' or 'Bottleneck'.")
        super(ResNet9_P3D, self).__init__()
        if norm_layer is None:
            norm_layer = nn.BatchNorm3d

        self.inplanes = channels[0]

        self.conv1 = nn.Conv3d(in_channel, self.inplanes,
                               kernel_size=3, stride=1, padding=1, bias=False)
        self.bn1 = norm_layer(self.inplanes)
        self.relu = nn.ReLU(inplace=True)
        
        self.layer1 = self._make_layer(block, channels[0], layers[0], stride=strides[0], norm_layer=norm_layer)
        self.layer2 = self._make_layer(block, channels[1], layers[1], stride=strides[1], norm_layer=norm_layer)
        self.layer3 = self._make_layer(block, channels[2], layers[2], stride=strides[2], norm_layer=norm_layer)
        self.layer4 = self._make_layer(block, channels[3], layers[3], stride=strides[3], norm_layer=norm_layer)

        
        
        self.avgpool = nn.AdaptiveAvgPool3d((1, 1, 1))
        self.global_fusion = MultiScaleTemporalFusion()

        self.block2_conv = self.layer2[0].conv2d
        self.block3_conv = self.layer3[0].conv2d
        self.block4_conv = self.layer4[0].conv2d

        # æ”¾åœ¨ ResNet9_P3D.__init__ ä¸­
        self.TP = PackSequenceWrapper(lambda x, **kwargs: torch.max(x, **kwargs)[0])



        self.branch = ProgressiveFusionBranch(
            conv2=self.block2_conv,
            conv3=self.block3_conv,
            conv4=self.block4_conv,
            fuse2=FusionModule(128),
            fuse3=FusionModule(256),
            fuse4=FusionModule(512),
            layer3=self.layer3,
            layer4=self.layer4
        )

    def _make_layer(self, block, planes, blocks, stride, norm_layer):
        if isinstance(stride, int):
            stride = (1, stride, stride)
        elif isinstance(stride, list):
            stride = tuple(stride)

        downsample = None
        if stride != (1, 1, 1) or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv3d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                norm_layer(planes * block.expansion),
            )

        layers = [block(self.inplanes, planes, stride=stride,
                        downsample=downsample, norm_layer=norm_layer)]
        self.inplanes = planes * block.expansion
        for _ in range(1, blocks):
            layers.append(block(self.inplanes, planes, stride=(1, 1, 1), norm_layer=norm_layer))

        return nn.Sequential(*layers)

    def forward(self, x, seqL=None):
        x = self.relu(self.bn1(self.conv1(x)))

        x1 = self.layer1(x)
        x2 = self.layer2(x1)
#         print(f"After conv2 dtype: {x2.dtype}")  

        # åˆ†æ”¯èåˆè¾“å‡º
        x2_out,x3_out,x_dyn = self.branch(x1, x2)

        # æ—¶é—´æ± åŒ–æå–å¤šå°ºåº¦é™æ€ç‰¹å¾
        e1 = self.TP(x2_out, seqL, options={"dim": 2, "keepdim": True})
        e2 = self.TP(x3_out, seqL, options={"dim": 2, "keepdim": True})
#         print("e1ç‰¹å¾ç»´åº¦å¤§å°ï¼š",e1.shape)
#         print("e2ç‰¹å¾ç»´åº¦å¤§å°ï¼š",e2.shape)
#         print("x_dynç‰¹å¾ç»´åº¦å¤§å°ï¼š",x_dyn.shape)
        # å¤šå°ºåº¦æ—¶é—´èåˆè¾“å‡º
        fused_feat = self.global_fusion(e1, e2, x_dyn)
#         print("fused_featçš„ç‰¹å¾ç»´åº¦å¤§å°ï¼š",fused_feat.shape)
        return fused_feat
